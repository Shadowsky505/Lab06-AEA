# Usar una imagen base con Python y Spark
FROM openjdk:11-jre-slim

# Instalar dependencias necesarias
RUN apt-get update && \
    apt-get install -y python3 python3-pip curl && \
    apt-get clean

# Instalar Apache Spark
ENV SPARK_VERSION=3.3.0
RUN curl -sL "https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.2.tgz" | tar -xz -C /opt/ && \
    ln -s /opt/spark-${SPARK_VERSION}-bin-hadoop3.2 /opt/spark

# Instalar dependencias de Python
RUN pip3 install fastapi polars pyspark uvicorn

# Establecer el directorio de trabajo
WORKDIR /app

# Copiar el c√≥digo de la API
COPY ./app /app

# Exponer el puerto
EXPOSE 8000

# Comando para correr la API
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
